---
title: "베이즈 필터와 파티클 필터"
category: math
date:
---

이번에는 미뤄뒀던 파티클 필터를 공부하기로 했습니다. 파티클 필터는 베이즈 필터의 일종이므로 베이즈 필터와 이것이 어떻게 작동하는지부터 알아보겠습니다.

# (신호) 필터

다양한 필터를 이해하기 위해서는 먼저 필터가 무엇인지를 이해해야 합니다. 신호처리 및 제어공학에서 필터란, 그 이름에서 알 수 있듯 다양한 신호가 중첩된 신호에서 원하는 신호만을 통과시키는 장치나 그런 구조를 의미합니다.

제어나 신호처리의 관점에서는 모든 신호를 원하는 신호와 잡음으로 나누어 생각할 수 있습니다. 만약 이상적으로 신호의 전달이 이루어진다면 필터가 필요없겠지만, 현실에서는 측정의 불확실성을 비롯한 다양한 이유로 잡음이 포함되기 때문에 이러한 필터가 필요합니다.

이러한 관점에서 필터는 측정된 신호의 값으로부터 실제 신호의 값을 추정하는 장치로 생각할 수 있습니다. 그런데 당연하게도 어떤 시점에 측정한 하나의 값만 가지고서는 그 값이 참값인지를 알 수가 없습니다. 그렇기 때문에 다른 시간의 측정값들을 함께 활용하여 어떤 시점의 참값을 추정하는 것이 필요합니다.

만약 측정 시점보다 한참 이후에 추정을 진행한다면 추정하려는 시점보다 미래의 측정값들을 이용할 수 있습니다. 그러나 많은 경우에는 실시간으로 추정을 진행해야 하기 때문에 보통 이전 시점 및 현재 시점의 측정값만을 이용해서 추정을 진행하게 됩니다.

# 베이즈 필터

베이즈 필터는 이러한 필터의 일종으로, 현재 시점 및 그 이전 시점의 측정값들 $z_t, z_{t-1}, \cdots, z_0$로부터 참값 $x_t$를 추정하는 것을 다음과 같이 조건부 확률을 통해서 표현합니다.

$$
p(x_t | z_t, z_{t-1}, \cdots, z_0)
$$

> $p(x_0, x_1, \cdots, x_n)$은 $x_0, x_1, \cdots, x_n$의 결합확률분포를 의미합니다. 즉, $p(x_0 \cap x_1 \cap \cdots \cap x_n)$을 나타내는 것입니다.

이것은 측정값들이 주어질 때 참값의 확률분포를 구하는 것으로 생각할 수 있습니다. 그리고 이 확률분포의 평균이나 최빈값을 참값으로 사용할 수 있습니다.

## 마르코프 체인

그런데 이 식은 모든 측정값들을 매개변수로 가지고 있기 때문에 시간이 지남에 따라 측정값이 늘어나면서 식이 계속 늘어나게 됩니다. 이러한 식은 다뤄야 할 상태 공간이 기하급수적으로 커지기 때문에 애초에 식을 세우기도 어려울 뿐더러 현실적으로 이용하기가 불가능합니다. 그래서 시스템이 **마르코프 체인**이라는 좋은 조건을 따른다는 가정을 추가합니다.

어떤 시스템이 마르코프 체인이라는 것은 어떤 시스템의 그 다음 상태가 과거와는 무관하게 현재 상태에만 의존한다는 것을 의미합니다. 다행스럽게도 현실에서 대부분의 시스템은 이러한 가정을 따릅니다. 예를 들어서 하늘로 던져진 물체를 가정해보면 그 물체가 그 다음 순간에 이동할 위치는 현재의 위치와 속도에만 의존합니다. 그 물체가 땅에서 발사되었든, 아니면 하늘에서 떨어진 후에 땅에 튕긴 것이든, 누군가 일부러 던진 것이든, 어떤 순간에 동일한 위치와 속도를 가진다면 그 다음 순간 위치의 확률분포는 동일할 것입니다.

이러한 가정은 다음과 같이 표현할 수 있습니다.

$$
p(x_t | x_{t-1}, x_{t-2}, \cdots, x_0) = p(x_t | x_{t-1})
$$

## 은닉 마르코프 체인

그런데 실제로는 정확한 상태를 직접 측정할 수 있는 것은 아니며, 참값에 의존하는 다른 값인 관측값만을 얻을 수 있습니다. 이러한 시스템을 은닉 마르코프 체인이라고 합니다. 이러한 시스템에서는 이전의 관측값들로부터 참값을 추정하게 됩니다. 즉, 관측값 $z_i$에 따른 참값의 확률분포는 다음과 같이 표현됩니다.

$$
p(x_t | z_t, z_{t-1}, \cdots, z_0)
$$

## 조건부 독립

그런데 위 확률분포를 직접 계산하기는 대단히 어렵습니다. 그러나 알려진 확률분포로부터 베이즈 정리를 사용하여 위 값을 계산할 수 있습니다.

베이즈 정리를 마르코프 체인에 적용하기 위해서는 마르코프 체인을 조건부 독립(conditional independency)의 관점에서 해석해야 합니다. 조건부 독립이란 **어떤 사건 $A$가 주어졌을 때** 사건 $B$와 $C$가 독립이라는 것을 의미합니다. 이것은 다음과 같이 다양한 방법으로 표현할 수 있습니다.

- $p(B, C|A) = p(B|A)p(C|A)$: $A$가 주어졌을 때 $B$와 $C$의 결합확률분포는 $B$와 $C$의 확률분포의 곱과 같다.
- $p(A \cap B \cap C) = p(A \cap B) p(A \cap C) / P(A)$: 위 식을 조건부 확률의 정의에 따라 전개한 것

이는 조건부 확률로도 표현할 수 있습니다.

- $p(B|A, C) = p(B|A)$: $A$가 주어졌을 때 $B$의 확률분포는 $C$에 영향을 받지 않는다.
- $p(C|A, B) = p(C|A)$: $A$가 주어졌을 때 $C$의 확률분포는 $B$에 영향을 받지 않는다.

이때 중요한 것은 $B$와 $C$가 독립이라는 것은 $A$가 주어졌을 때에만 성립한다는 것입니다. 즉, 일반적으로 $B$와 $C$는 독립이 아닙니다.

이것을 마르코프 체인에 적용하기 위해서는 $B=x_t, C=x_{t-1}, A=x_{t-2}, \cdots, x_0$으로 두면 됩니다. 이로부터 다음 식을 얻습니다.

$$
p(x_t|x_{t-1}, x_{t-2}, \cdots, x_0) = p(x_t|x_{t-1})
$$

또한 두 변수 $A, B$가 다른 변수 $C$에 대해 조건부 독립인 경우 다음이 성립합니다.

$$
p(A | B) = \int p (A |C) p(C|B) dC
$$

증명은 아래와 같습니다.

$A$와 $B$가 조건부 독립이므로

$$
p(A | C) = p(A | B, C)
$$

이것을 위 식에 대입하면

$$
p(A | B) = \int p(A | B, C) p(C | B) dC
$$

우변을 조건부 확률의 정의에 따라 전개하면

$$
= \int \frac{p(A \cap B \cap C)}{p(B \cap C)} \frac{p(C \cap B)}{p(B)} dC\\
$$

$$
= \int \frac{p(A \cap B \cap C)}{p(B)} dC
$$

전체 확률의 법칙(law of total probability)에 의해

$$
= \frac{p(A \cap B)}{p(B)} = p(A | B)
$$

## 베이즈 정리

다시 한번 구하고자 하는 확률분포를 상기해보면 아래와 같습니다.

$$
p(x_t | z_t, z_{t-1}, \cdots, z_0)
$$

그리고 다음과 같은 두 확률분포를 이미 알고 있다고 가정합니다.

- $p(z_t | x_t)$: 참값이 주어졌을 때 측정값의 확률분포
- $p(x_t | x_{t-1})$: 이전 참값이 주어졌을 때 참값의 확률분포

이 두 확률분포를 각각 **측정 모델**과 **시스템 모델**이라고 합니다. 이 두 확률분포를 이용하여 참값의 확률분포를 계산할 수 있습니다.

먼저 구하고자 하는 식에 그대로 베이즈 정리를 적용하면

$$
p(x_t | z_t, z_{t-1}, \cdots, z_0) = \frac{p(z_t, z_{t-1}, \cdots, z_0 | x_t) p(x_t)}{p(z_t, z_{t-1}, \cdots, z_0)}
$$

이때 $z_t$는 오직 $x_t$에만 의존한다고 가정하였으므로 $x_t$가 아닌 임의의 확률변수 $k$에 대해 조건부 독립이 성립합니다. 따라서 다음과 같이 표현할 수 있습니다.

$$
\forall k \neq x_t, p(z_t, k | x_t) = p(z_t | x_t) p(k | x_t)
$$

이로부터 $k=z_{t-1}, z_{t-2}, \cdots, z_0$로 두면

$$
p(z_t, z_{t-1}, \cdots, z_0 | x_t) = p(z_t | x_t) p(z_{t-1}, z_{t-2}, \cdots, z_0 | x_t)
$$

이것을 다시 베이즈 정리에 적용하면

$$
= \frac{p(z_t | x_t) p(z_{t-1}, z_{t-2}, \cdots, z_0 | x_t) p(x_t)}{p(z_t, z_{t-1}, \cdots, z_0)}
$$

그런데 $p(z_{t-1}, z_{t-2}, \cdots, z_0 | x_t) p(x_t) = p(z_{t-1}, z_{t-2}, \cdots, z_0, x_t)$이므로

$$
= \frac{p(z_t | x_t) p(z_{t-1}, z_{t-2}, \cdots, z_0, x_t)}{p(z_t, z_{t-1}, \cdots, z_0)}
$$

분모와 분자를 각각 $p(z_{t-1}, z_{t-2}, \cdots, z_0)$으로 나누면

$$
= \frac{p(z_t | x_t) p(x_t | z_{t-1}, z_{t-2}, \cdots, z_0)}{p(z_t | z_{t-1}, z_{t-2}, \cdots, z_0)}
$$

이때 분모 $p(z_t | z_{t-1}, z_{t-2}, \cdots, z_0)$은 앞서 조건부 독립 섹션에서 다뤘던 성질에 의해 다음과 같이 분해되므로

$$
p(z_t | z_{t-1}, z_{t-2}, \cdots, z_0) = \int p(z_t | x_t) p(x_t | z_{t-1}, z_{t-2}, \cdots, z_0) dx_t
$$

이것을 다시 식에 대입하면 다음을 얻습니다.

$$
p(x_t | z_t, z_{t-1}, \cdots, z_0) = \frac{p(z_t | x_t) p(x_t | z_{t-1}, z_{t-2}, \cdots, z_0)}{\int p(z_t | x_t) p(x_t | z_{t-1}, z_{t-2}, \cdots, z_0) dx_t}
$$

이 식에서 알지 못하는 부분은 $p(x_t | z_{t-1}, z_{t-2}, \cdots, z_0)$입니다. 이것은 $x_t$가 오직 $x_{t-1}$에만 의존한다는 가정에 의해 $z_{t-1}, z_{t-2}, \cdots, z_0$과 조건부 독립이 되므로, 앞에서 보인 것과 마찬가지의 방식으로 다음과 같이 분해할 수 있습니다.

$$
p(x_t | z_{t-1}, z_{t-2}, \cdots, z_0) = \int p(x_t | x_{t-1}) p(x_{t-1} | z_{t-1}, z_{t-2}, \cdots, z_0) dx_{t-1}
$$

이로부터 다음과 같은 재귀적인 추정을 수행할 수 있습니다.

1. $p(x_{t-1} | z_{t-1}, z_{t-2}, \cdots, z_0)$를 알고 있다고 가정합니다.
1. 이로부터 $p(x_t | z_{t-1}, z_{t-2}, \cdots, z_0)$를 계산합니다. 이것은 $t-1$ 시점까지의 정보를 바탕으로 $t$ 시점의 참값의 확률분포를 계산하는 것이므로 이 과정을 **추정** 또는 **예측** (prediction)이라고 합니다.
1. 이로부터 $p(x_t | z_t, z_{t-1}, \cdots, z_0)$를 계산합니다. 이것은 추정값에 새로운 측정값을 반영하여 참값의 확률분포를 재계산하는 것이므로 이 과정을 **업데이트** (update)라고 합니다.

물론 맨 처음 단계에서는 초기 추정값 $p(x_0)$를 알고 있어야 합니다. $p(x_0)$은 아무런 정보도 없는 경우의 확률분포이므로 균등확률분포나 정규분포 등을 사용할 수 있으며 더 정밀한 초기 추정값을 사용할수록 이후 추정의 정확도가 높아집니다.

위와 같이 참값의 확률분포를 추정하는 방식을 **베이즈 필터**라고 합니다.

# 파티클 필터

그런데 베이즈 필터의 계산 과정에는 적분이 들어갑니다. 이 적분은 기본적으로 두 함수의 곲으로 주어지는데다 보통은 그 함수들도 수치적으로 정의된 함수라서 해석적으로 계산하는 것이 불가능합니다. 따라서 수치적으로 계산하는 수밖에 없는데, 계산량은 상태 공간에 비례하므로 상태 공간의 차원이 커지면 계산량도 대단히 커집니다.

이에 따라 **몬테카를로 샘플링(Monte Carlo sampling)** 을 이용하여 베이즈 필터를 근사하는 방법이 제안되었습니다. 이것을 **파티클 필터(particle filter)** 라고 합니다.

TODO: Importance sampling 개념 간략히 정리, 이로부터 파티클 필터가 계산가능함을 설명 (proposal probability를 $p(x_t|x_{t-1})$로 적당히 선택하면 쉽게 가능), 그런데 importance sampling을 하면 샘플 개수가 줄어드는 문제, 샘플 분산이 줄어드는 문제 등을 설명, 이를 해결하기 위해 resampling을 사용한다고 설명
