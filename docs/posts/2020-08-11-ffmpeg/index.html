<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="description" content="Unknownpgr&#x27;s Blog"/><link rel="icon" href="/favicon.png"/><meta property="og:title" content="Unknownpgr&#x27;s Blog"/><meta property="og:url" content="https://unknownpgr.com"/><meta property="og:image" content="/logo.png"/><title>Unknownpgr : HTTP 웹캠 라이브 스트리밍 구현하기</title><meta property="og:title" content="Unknownpgr: HTTP 웹캠 라이브 스트리밍 구현하기"/><meta name="next-head-count" content="9"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous"/><link rel="preload" href="/_next/static/css/2e1e96656eb72d38.css" as="style"/><link rel="stylesheet" href="/_next/static/css/2e1e96656eb72d38.css" data-n-g=""/><link rel="preload" href="/_next/static/css/bb77c4673c8d1507.css" as="style"/><link rel="stylesheet" href="/_next/static/css/bb77c4673c8d1507.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-3b5a00d5d7e8d93b.js" defer=""></script><script src="/_next/static/chunks/main-1cd5204319a0e744.js" defer=""></script><script src="/_next/static/chunks/pages/_app-b2167bfc37ff4f0b.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-ef770e12683a69af.js" defer=""></script><script src="/_next/static/5MaiEKf8969grUIlJXEoz/_buildManifest.js" defer=""></script><script src="/_next/static/5MaiEKf8969grUIlJXEoz/_ssgManifest.js" defer=""></script></head><body><div id="__next"><nav class="nav_nav__XkaAa"><a class="nav_logo__drPJg" href="/">[UNKNOWN-PGR]</a><a class="nav_link__KHlEJ" href="/">Home</a><a class="nav_link__KHlEJ" href="/categories/">Categories</a><a class="nav_link__KHlEJ" href="/about/">About</a></nav><main class="layout_main__j5lor"><div><h1 class="post_title__aQveY">HTTP 웹캠 라이브 스트리밍 구현하기</h1><div class="post_info__7zaFc"><i>live streaming<!-- --> / <!-- -->2020-08-11T04:03:18.931Z</i></div><main class="post_main__mmVmG"><p>이번에 어떤 프로젝트에 참여하면서 라이브 스트리밍을 구현해야 할 일이 생겼습니다. 어떤 카메라로부터 영상을 받아서 그것을 웹브라우저에서 확인할 수 있게 해야 하는 건데요. 구글링을 하면서 찾아보니, 이걸 깔끔하게 구현해주는 마땅한 솔루션이 없었습니다. 그래서 이걸 직접 구현해보기로 했습니다.</p>
<ul>
<li><a href="https://github.com/unknownpgr/node-webcam-streaming">깃허브 리포</a></li>
</ul>
<p>영상, 특히 라이브 스트리밍을 할 때 생기는 가장 큰 문제점은 도대체 어떻게 영상을 받을지, 그리고 어떻게 스트리밍할지입니다. 다행스럽게도 저는 이전에 비슷한 경험을 몇 번 해 보아서 두 가지 솔루션을 알고 있었습니다.</p>
<ol>
<li>OpenCV등을 이용, 카메라 <u>영상을 이미지로 변환 후 클라이언트에게 전송</u>. 클라이언트는 이미지를 계속 업데이트하여 보여줌. 그러므로 실은 영상이 아니라 빠르게 바뀌는 이미지에 불과함.</li>
<li><u><a href="https://ffmpeg.org/">FFmpeg</a></u>를 사용.</li>
</ol>
<p>언듯 생각하기에는 1번 솔루션은 상당히 단순무식해보입니다. 그러나 실제로 해 보면 꽤 영상 퀄리티와 FPS가 괜찮은데다, 구현하기도 편하고(인코딩 관련 고생할 필요가 없습니다), 특히 영상에 어떤 처리를 한 후 전송해야 할 때 유용합니다. 그러나 소리를 전송할 수 없는 단점이 있습니다. 그래서 이번에는 2번 솔루션을 구현해보기로 했습니다.</p>
<h1>이전에 했던 것</h1>
<p>예전에 FFmpeg를 사용할 때에는 실시간으로 영상 전송이 가능한 프로토콜을 잘 몰랐습니다. 그래서 상당히 로우레벨로 접근해야만 했었는데, 다음과 같은 방법을 사용했습니다.</p>
<ol>
<li>FFmpeg를 사용하여 웹캠 영상을 webm형식으로 변환, 출력 스트림을 <code>stdout</code>으로 pipe하면 Node.js 서버에서 그것을 받음.</li>
<li>Node.js서버에서는 받은 스트림을 버퍼에 담음.</li>
<li>버퍼가 다 차면 버퍼를 큐에 넣음.</li>
<li>만약 클라이언트가 영상을 요구할 경우, 서버는 큐에서 버퍼를 꺼내서 클라이언트에게 전송함.</li>
<li>클라이언트는 HTML5의 <a href="https://developer.mozilla.org/en-US/docs/Web/API/MediaSource">Media Source API</a>를 이용하여 버퍼를 video tag에 추가함.</li>
</ol>
<p>커밋 기록을 보니 이걸 구현한 게 19년 2월이었으니까, 대충 고등학교 졸업하기 직전이네요. 지금 와서 보니 상당히 비효율적인 구현입니다. 애초에 두 사람이 동시에 스트리밍을 받을 수 없는 구조네요.</p>
<h1>이번에 한 것</h1>
<p>이번에는 저렇게 하지 않고, FFmpeg의 기능을 충분히 이용하기로 했습니다. 코드가 정말 간단해서, 어쩌면 직접 <a href="https://github.com/unknownpgr/node-webcam-streaming/blob/master/index.js">소스코드</a>를 보시는 편이 더 이해가 잘 될 수도 있습니다.</p>
<ul>
<li>FFmpeg는 출력을 HTTP Live Streaming(HLS)로 할 수 있는데, 이렇게 하면 출력으로 <code>.m3u8</code>파일과 여러 <code>.ts</code>파일들이 생깁니다. <code>.ts</code>파일은 영상을 조각으로 나눠놓은 segment파일이며, <code>.m3u8</code>파일은 <code>.ts</code>파일들의 정보를 담고 있는 메타 파일입니다.</li>
<li>Node.js서버는 단지 저 파일들을 정적으로 서비스하기만 합니다.</li>
<li>클라이언트는 <a href="https://github.com/video-dev/hls.js#getting-started">hls.js</a> 라이브러리를 사용하여 영상을 <code>video</code>태그로 볼 수 있도록 합니다. 물론 일부 브라우저는 외부 라이브러리 없이 <code>video</code>태그의 <code>src</code>에 <code>.m3u8</code>파일을 넘겨줘도 잘 재생해줍니다. 그런데 가장 메이저한 브라우저인 크롬 데스크톱에서 이게 안 돼서 그냥 라이브러리를 쓰기로 했습니다.</li>
</ul>
<p>아래는 Node.js에서 실행하는 FFmpeg 커맨드를 그대로 옮겨 온 것입니다. FFmpeg가 설치되어있을 경우 터미널에 바로 입력하면 작동합니다.</p>
<pre><code class="language-bash">ffmpeg -f v4l2 -i /dev/video0 -c:v libx264 -crf 23 -pix_fmt yuv420p -hls_time 2 -hls_list_size 5 -hls_delete_threshold 1 -hls_flags delete_segments -f hls public/video.m3u8
</code></pre>
<p>인자에 대해 하나씩 알아보도록 하겠습니다. 소스코드에도 설명이 있습니다.</p>
<ul>
<li><code>-f v4l2</code>: 입력 포맷을 v4l2(웹캠)으로 한다.</li>
<li><code>-i /dev/video0</code>: <code>/dev/video0</code>(웹캠)에서 영상을 읽어온다.</li>
<li><code>-c:v libx264</code>: 출력 비디오 포맷을 <code>libx264</code>로 한다.</li>
<li><code>-crf 23</code>: Constant Rate Factor (CRF)를 23으로 한다. CRF는 영상의 품질과 관련된 인자로, 낮을수록 화질이 좋아지며, 23이 기본값이다.</li>
<li><code>-pix_fmt yuv420p</code>: 픽셀 포맷을 Y:U:V = 4:2:0으로 설정한다. 이는 어떤 플레이어들이 이 포맷밖에 지원하지 않는 경우가 있기 때문으로, 실제로 이렇게 하지 않으면 크롬에서 비디오가 깨진다. <a href="https://trac.ffmpeg.org/wiki/Encode/H.264">설명 참조</a>.</li>
<li><code>-hls_time 2</code>: ts파일의 길이를 2초로 한다. (이유는 알 수 없지만 제 컴퓨터에서는 이 옵션이 제대로 작동하지 않고, 파일 길이가 약 8초였습니다.)</li>
<li><code>-hls_list_size 5</code>: <code>.m3u8</code>파일에 포함할 <code>.ts</code>파일 리스트의 길이를 최대 5로 설정한다. 이는 실시간 스트리밍이기 때문에 필요한 옵션으로, 오래 전에 생성된 것들까지 포함해서 모든 segment들을 <code>.m3u8</code>파일에 포함하면 비효율적이므로 최근 5개의 segment만 포함하는 것이다.</li>
<li><code>-hls_delete_threshold 1</code>: <code>.m3u8</code>에 포함되지 않은 segment를 1개까지 허용한다. 추후 설명.</li>
<li><code>-hls_flags delete_segments</code>: 오래된 segment를 삭제한다.</li>
<li><code>-f hls public/video.m3u8</code>: 출력 파일 이름. 이렇게 하면 <code>.ts</code>파일들은 <code>video.ts</code>와 같이 생성된다.</li>
</ul>
<p>위의 옵션 중 <code>-hls_list_size</code>, <code>-hls_delete_threshold</code>, <code>-hls_flags delete_segments</code>는 세 개가 한 세트인 옵션입니다. 약간 설명이 복잡한데, <code>-hls_flags delete_segments</code>옵션은 위에서 언급했듯 오래된 segment들을 지우는 옵션이고, <code>-hls_delete_threshold</code>옵션은 <code>.m3u8</code>에 포함되지 않은 segment들의 경우, 몇 개까지를 오래된 것으로 생각할 것인지를 지정하는 옵션입니다.</p>
<p>예를 들어, <code>hls_list_size</code>가 7이고, <code>hls_delete_threshold</code>가 3이라고 가정합니다. 그러면 <code>.m3u8</code>에는 n, n+1, n+2...n+6까지 총 7개의 segment에 대한 정보가 있을 것입니다. 이때 <code>hls_delete_threshold</code>가 3이기 때문에, n-1, n-2, n-3까지의 segment는 오래된 것으로 간주하지 않고, n-4부터를 오래된 것으로 간주합니다. 그러므로 <code>.ts</code>파일은 항상 10개가 존재하게 됩니다.</p>
<p><code>hls_delete_threshold</code>옵션이 필요한 이유는, 비록 <code>.m3u8</code>의 segment 리스트에서는 특정 segment가 지워졌지만, 인터넷 속도 등의 문제로 해당 segment를 계속 다운받고 있는 클라이언트가 있을 수도 있기 때문입니다.</p>
<p>이렇게 옵션을 주면 파일들이 실시간으로 업데이트되며, 클라이언트는 일반 HLS파일을 재생하듯 실시간 영상을 재생할 수 있습니다. 아래는 스크린샷입니다.</p>
<p><img src="/imgs/b1e1387535b43472a86e79e1db83a560.png" alt="screenshot"></p>
<p>웹캠에서 찍은 영상을 HTML로 스트리밍하여 볼 수 있습니다. (URL은 개인 서버를 사용 중이라 지웠습니다.)</p>
<h1>참고문헌</h1>
<ul>
<li>
<p><a href="https://superuser.com/questions/714974/convert-rtmp-streaming-to-hls-streaming-using-ffmpeg">FFmpeg RTMP HLS변환 방법</a></p>
<ul>
<li>
<p>첫 번째 답변이 도움이 많이 되었습니다. 단, 아래 옵션 중 <code>-hls_wrap</code>은 deprecated되었습니다. 대신 제가 사용한 것처럼 <code>-hls_delete_threshold 1 -hls_flags delete_segments</code>를 사용하시면 됩니다.</p>
<pre><code class="language-bash">ffmpeg -v verbose -i rtmp://host:port/stream -c:v libx264 -c:a aac -ac 1 -strict -2 -crf
18 -profile:v baseline -maxrate 400k -bufsize 1835k -pix_fmt yuv420p -flags -global_header -hls_time
10 -hls_list_size 6 -hls_wrap 10 -start_number 1 pathToFolderYouWantTo/streamName.m3u8
</code></pre>
</li>
</ul>
</li>
<li>
<p><a href="https://superuser.com/questions/677576/what-is-crf-used-for-in-ffmpeg">CRF 설명</a></p>
</li>
<li>
<p><a href="https://trac.ffmpeg.org/wiki/Capture/Webcam">FFmpeg에서 웹캠 입력받는 방법</a></p>
</li>
<li>
<p><a href="https://linuxize.com/post/how-to-install-ffmpeg-on-centos-8/">CentOS에서 FFmpeg 설치 방법</a></p>
</li>
</ul>
</main><h1>Posts in <!-- -->live streaming<!-- --> category</h1><ul><li class="post_listSelected__vtK9Y"><a href="/posts/2020-08-11-ffmpeg/">HTTP 웹캠 라이브 스트리밍 구현하기</a></li></ul><div></div></div></main><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5695206410217978" data-ad-slot="9579212903" data-ad-format="auto" data-full-width-responsive="true"></ins><footer class="footer_footer__E_Uzl">© 2020 Copyright : Unknownpgr</footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"HTTP 웹캠 라이브 스트리밍 구현하기","category":"live streaming","date":"2020-08-11T04:03:18.931Z","postStr":"---\ntitle: HTTP 웹캠 라이브 스트리밍 구현하기\ncategory: live streaming\ndate: 2020-08-11T04:03:18.931Z\n\n---\n\n이번에 어떤 프로젝트에 참여하면서 라이브 스트리밍을 구현해야 할 일이 생겼습니다. 어떤 카메라로부터 영상을 받아서 그것을 웹브라우저에서 확인할 수 있게 해야 하는 건데요. 구글링을 하면서 찾아보니, 이걸 깔끔하게 구현해주는 마땅한 솔루션이 없었습니다. 그래서 이걸 직접 구현해보기로 했습니다.\n\n- [깃허브 리포](https://github.com/unknownpgr/node-webcam-streaming)\n\n영상, 특히 라이브 스트리밍을 할 때 생기는 가장 큰 문제점은 도대체 어떻게 영상을 받을지, 그리고 어떻게 스트리밍할지입니다. 다행스럽게도 저는 이전에 비슷한 경험을 몇 번 해 보아서 두 가지 솔루션을 알고 있었습니다.\n\n1. OpenCV등을 이용, 카메라 \u003cu\u003e영상을 이미지로 변환 후 클라이언트에게 전송\u003c/u\u003e. 클라이언트는 이미지를 계속 업데이트하여 보여줌. 그러므로 실은 영상이 아니라 빠르게 바뀌는 이미지에 불과함.\n2. \u003cu\u003e[FFmpeg](https://ffmpeg.org/)\u003c/u\u003e를 사용.\n\n언듯 생각하기에는 1번 솔루션은 상당히 단순무식해보입니다. 그러나 실제로 해 보면 꽤 영상 퀄리티와 FPS가 괜찮은데다, 구현하기도 편하고(인코딩 관련 고생할 필요가 없습니다), 특히 영상에 어떤 처리를 한 후 전송해야 할 때 유용합니다. 그러나 소리를 전송할 수 없는 단점이 있습니다. 그래서 이번에는 2번 솔루션을 구현해보기로 했습니다.\n\n# 이전에 했던 것\n\n예전에 FFmpeg를 사용할 때에는 실시간으로 영상 전송이 가능한 프로토콜을 잘 몰랐습니다. 그래서 상당히 로우레벨로 접근해야만 했었는데, 다음과 같은 방법을 사용했습니다.\n\n1. FFmpeg를 사용하여 웹캠 영상을 webm형식으로 변환, 출력 스트림을 `stdout`으로 pipe하면 Node.js 서버에서 그것을 받음.\n2. Node.js서버에서는 받은 스트림을 버퍼에 담음.\n3. 버퍼가 다 차면 버퍼를 큐에 넣음.\n4. 만약 클라이언트가 영상을 요구할 경우, 서버는 큐에서 버퍼를 꺼내서 클라이언트에게 전송함.\n5. 클라이언트는 HTML5의 [Media Source API](https://developer.mozilla.org/en-US/docs/Web/API/MediaSource)를 이용하여 버퍼를 video tag에 추가함.\n\n커밋 기록을 보니 이걸 구현한 게 19년 2월이었으니까, 대충 고등학교 졸업하기 직전이네요. 지금 와서 보니 상당히 비효율적인 구현입니다. 애초에 두 사람이 동시에 스트리밍을 받을 수 없는 구조네요.\n\n# 이번에 한 것\n\n이번에는 저렇게 하지 않고, FFmpeg의 기능을 충분히 이용하기로 했습니다. 코드가 정말 간단해서, 어쩌면 직접 [소스코드](https://github.com/unknownpgr/node-webcam-streaming/blob/master/index.js)를 보시는 편이 더 이해가 잘 될 수도 있습니다.\n\n- FFmpeg는 출력을 HTTP Live Streaming(HLS)로 할 수 있는데, 이렇게 하면 출력으로 `.m3u8`파일과 여러 `.ts`파일들이 생깁니다. `.ts`파일은 영상을 조각으로 나눠놓은 segment파일이며, `.m3u8`파일은 `.ts`파일들의 정보를 담고 있는 메타 파일입니다.\n- Node.js서버는 단지 저 파일들을 정적으로 서비스하기만 합니다.\n- 클라이언트는 [hls.js](https://github.com/video-dev/hls.js#getting-started) 라이브러리를 사용하여 영상을 `video`태그로 볼 수 있도록 합니다. 물론 일부 브라우저는 외부 라이브러리 없이 `video`태그의 `src`에 `.m3u8`파일을 넘겨줘도 잘 재생해줍니다. 그런데 가장 메이저한 브라우저인 크롬 데스크톱에서 이게 안 돼서 그냥 라이브러리를 쓰기로 했습니다.\n\n아래는 Node.js에서 실행하는 FFmpeg 커맨드를 그대로 옮겨 온 것입니다. FFmpeg가 설치되어있을 경우 터미널에 바로 입력하면 작동합니다.\n\n```bash\nffmpeg -f v4l2 -i /dev/video0 -c:v libx264 -crf 23 -pix_fmt yuv420p -hls_time 2 -hls_list_size 5 -hls_delete_threshold 1 -hls_flags delete_segments -f hls public/video.m3u8\n```\n\n인자에 대해 하나씩 알아보도록 하겠습니다. 소스코드에도 설명이 있습니다.\n\n- `-f v4l2`: 입력 포맷을 v4l2(웹캠)으로 한다.\n- `-i /dev/video0`: `/dev/video0`(웹캠)에서 영상을 읽어온다.\n- `-c:v libx264`: 출력 비디오 포맷을 `libx264`로 한다.\n- `-crf 23`: Constant Rate Factor (CRF)를 23으로 한다. CRF는 영상의 품질과 관련된 인자로, 낮을수록 화질이 좋아지며, 23이 기본값이다.\n- `-pix_fmt yuv420p`: 픽셀 포맷을 Y:U:V = 4:2:0으로 설정한다. 이는 어떤 플레이어들이 이 포맷밖에 지원하지 않는 경우가 있기 때문으로, 실제로 이렇게 하지 않으면 크롬에서 비디오가 깨진다. [설명 참조](https://trac.ffmpeg.org/wiki/Encode/H.264).\n- `-hls_time 2`: ts파일의 길이를 2초로 한다. (이유는 알 수 없지만 제 컴퓨터에서는 이 옵션이 제대로 작동하지 않고, 파일 길이가 약 8초였습니다.)\n- `-hls_list_size 5`: `.m3u8`파일에 포함할 `.ts`파일 리스트의 길이를 최대 5로 설정한다. 이는 실시간 스트리밍이기 때문에 필요한 옵션으로, 오래 전에 생성된 것들까지 포함해서 모든 segment들을 `.m3u8`파일에 포함하면 비효율적이므로 최근 5개의 segment만 포함하는 것이다.\n- `-hls_delete_threshold 1`: `.m3u8`에 포함되지 않은 segment를 1개까지 허용한다. 추후 설명.\n- `-hls_flags delete_segments`: 오래된 segment를 삭제한다.\n- `-f hls public/video.m3u8`: 출력 파일 이름. 이렇게 하면 `.ts`파일들은 `video.ts`와 같이 생성된다.\n\n위의 옵션 중 `-hls_list_size`, `-hls_delete_threshold`, `-hls_flags delete_segments`는 세 개가 한 세트인 옵션입니다. 약간 설명이 복잡한데, `-hls_flags delete_segments`옵션은 위에서 언급했듯 오래된 segment들을 지우는 옵션이고, `-hls_delete_threshold`옵션은 `.m3u8`에 포함되지 않은 segment들의 경우, 몇 개까지를 오래된 것으로 생각할 것인지를 지정하는 옵션입니다.\n\n예를 들어, `hls_list_size`가 7이고, `hls_delete_threshold`가 3이라고 가정합니다. 그러면 `.m3u8`에는 n, n+1, n+2...n+6까지 총 7개의 segment에 대한 정보가 있을 것입니다. 이때 `hls_delete_threshold`가 3이기 때문에, n-1, n-2, n-3까지의 segment는 오래된 것으로 간주하지 않고, n-4부터를 오래된 것으로 간주합니다. 그러므로 `.ts`파일은 항상 10개가 존재하게 됩니다.\n\n`hls_delete_threshold`옵션이 필요한 이유는, 비록 `.m3u8`의 segment 리스트에서는 특정 segment가 지워졌지만, 인터넷 속도 등의 문제로 해당 segment를 계속 다운받고 있는 클라이언트가 있을 수도 있기 때문입니다.\n\n이렇게 옵션을 주면 파일들이 실시간으로 업데이트되며, 클라이언트는 일반 HLS파일을 재생하듯 실시간 영상을 재생할 수 있습니다. 아래는 스크린샷입니다.\n\n![screenshot](screenshot.png)\n\n웹캠에서 찍은 영상을 HTML로 스트리밍하여 볼 수 있습니다. (URL은 개인 서버를 사용 중이라 지웠습니다.)\n\n# 참고문헌\n\n- [FFmpeg RTMP HLS변환 방법](https://superuser.com/questions/714974/convert-rtmp-streaming-to-hls-streaming-using-ffmpeg)\n\n  - 첫 번째 답변이 도움이 많이 되었습니다. 단, 아래 옵션 중 `-hls_wrap`은 deprecated되었습니다. 대신 제가 사용한 것처럼 `-hls_delete_threshold 1 -hls_flags delete_segments`를 사용하시면 됩니다.\n\n    ```bash\n    ffmpeg -v verbose -i rtmp://host:port/stream -c:v libx264 -c:a aac -ac 1 -strict -2 -crf\n    18 -profile:v baseline -maxrate 400k -bufsize 1835k -pix_fmt yuv420p -flags -global_header -hls_time\n    10 -hls_list_size 6 -hls_wrap 10 -start_number 1 pathToFolderYouWantTo/streamName.m3u8\n    ```\n\n- [CRF 설명](https://superuser.com/questions/677576/what-is-crf-used-for-in-ffmpeg)\n\n- [FFmpeg에서 웹캠 입력받는 방법](https://trac.ffmpeg.org/wiki/Capture/Webcam)\n\n- [CentOS에서 FFmpeg 설치 방법](https://linuxize.com/post/how-to-install-ffmpeg-on-centos-8/)","html":"\u003cp\u003e이번에 어떤 프로젝트에 참여하면서 라이브 스트리밍을 구현해야 할 일이 생겼습니다. 어떤 카메라로부터 영상을 받아서 그것을 웹브라우저에서 확인할 수 있게 해야 하는 건데요. 구글링을 하면서 찾아보니, 이걸 깔끔하게 구현해주는 마땅한 솔루션이 없었습니다. 그래서 이걸 직접 구현해보기로 했습니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/unknownpgr/node-webcam-streaming\"\u003e깃허브 리포\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e영상, 특히 라이브 스트리밍을 할 때 생기는 가장 큰 문제점은 도대체 어떻게 영상을 받을지, 그리고 어떻게 스트리밍할지입니다. 다행스럽게도 저는 이전에 비슷한 경험을 몇 번 해 보아서 두 가지 솔루션을 알고 있었습니다.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eOpenCV등을 이용, 카메라 \u003cu\u003e영상을 이미지로 변환 후 클라이언트에게 전송\u003c/u\u003e. 클라이언트는 이미지를 계속 업데이트하여 보여줌. 그러므로 실은 영상이 아니라 빠르게 바뀌는 이미지에 불과함.\u003c/li\u003e\n\u003cli\u003e\u003cu\u003e\u003ca href=\"https://ffmpeg.org/\"\u003eFFmpeg\u003c/a\u003e\u003c/u\u003e를 사용.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e언듯 생각하기에는 1번 솔루션은 상당히 단순무식해보입니다. 그러나 실제로 해 보면 꽤 영상 퀄리티와 FPS가 괜찮은데다, 구현하기도 편하고(인코딩 관련 고생할 필요가 없습니다), 특히 영상에 어떤 처리를 한 후 전송해야 할 때 유용합니다. 그러나 소리를 전송할 수 없는 단점이 있습니다. 그래서 이번에는 2번 솔루션을 구현해보기로 했습니다.\u003c/p\u003e\n\u003ch1\u003e이전에 했던 것\u003c/h1\u003e\n\u003cp\u003e예전에 FFmpeg를 사용할 때에는 실시간으로 영상 전송이 가능한 프로토콜을 잘 몰랐습니다. 그래서 상당히 로우레벨로 접근해야만 했었는데, 다음과 같은 방법을 사용했습니다.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eFFmpeg를 사용하여 웹캠 영상을 webm형식으로 변환, 출력 스트림을 \u003ccode\u003estdout\u003c/code\u003e으로 pipe하면 Node.js 서버에서 그것을 받음.\u003c/li\u003e\n\u003cli\u003eNode.js서버에서는 받은 스트림을 버퍼에 담음.\u003c/li\u003e\n\u003cli\u003e버퍼가 다 차면 버퍼를 큐에 넣음.\u003c/li\u003e\n\u003cli\u003e만약 클라이언트가 영상을 요구할 경우, 서버는 큐에서 버퍼를 꺼내서 클라이언트에게 전송함.\u003c/li\u003e\n\u003cli\u003e클라이언트는 HTML5의 \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/API/MediaSource\"\u003eMedia Source API\u003c/a\u003e를 이용하여 버퍼를 video tag에 추가함.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e커밋 기록을 보니 이걸 구현한 게 19년 2월이었으니까, 대충 고등학교 졸업하기 직전이네요. 지금 와서 보니 상당히 비효율적인 구현입니다. 애초에 두 사람이 동시에 스트리밍을 받을 수 없는 구조네요.\u003c/p\u003e\n\u003ch1\u003e이번에 한 것\u003c/h1\u003e\n\u003cp\u003e이번에는 저렇게 하지 않고, FFmpeg의 기능을 충분히 이용하기로 했습니다. 코드가 정말 간단해서, 어쩌면 직접 \u003ca href=\"https://github.com/unknownpgr/node-webcam-streaming/blob/master/index.js\"\u003e소스코드\u003c/a\u003e를 보시는 편이 더 이해가 잘 될 수도 있습니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFFmpeg는 출력을 HTTP Live Streaming(HLS)로 할 수 있는데, 이렇게 하면 출력으로 \u003ccode\u003e.m3u8\u003c/code\u003e파일과 여러 \u003ccode\u003e.ts\u003c/code\u003e파일들이 생깁니다. \u003ccode\u003e.ts\u003c/code\u003e파일은 영상을 조각으로 나눠놓은 segment파일이며, \u003ccode\u003e.m3u8\u003c/code\u003e파일은 \u003ccode\u003e.ts\u003c/code\u003e파일들의 정보를 담고 있는 메타 파일입니다.\u003c/li\u003e\n\u003cli\u003eNode.js서버는 단지 저 파일들을 정적으로 서비스하기만 합니다.\u003c/li\u003e\n\u003cli\u003e클라이언트는 \u003ca href=\"https://github.com/video-dev/hls.js#getting-started\"\u003ehls.js\u003c/a\u003e 라이브러리를 사용하여 영상을 \u003ccode\u003evideo\u003c/code\u003e태그로 볼 수 있도록 합니다. 물론 일부 브라우저는 외부 라이브러리 없이 \u003ccode\u003evideo\u003c/code\u003e태그의 \u003ccode\u003esrc\u003c/code\u003e에 \u003ccode\u003e.m3u8\u003c/code\u003e파일을 넘겨줘도 잘 재생해줍니다. 그런데 가장 메이저한 브라우저인 크롬 데스크톱에서 이게 안 돼서 그냥 라이브러리를 쓰기로 했습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e아래는 Node.js에서 실행하는 FFmpeg 커맨드를 그대로 옮겨 온 것입니다. FFmpeg가 설치되어있을 경우 터미널에 바로 입력하면 작동합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003effmpeg -f v4l2 -i /dev/video0 -c:v libx264 -crf 23 -pix_fmt yuv420p -hls_time 2 -hls_list_size 5 -hls_delete_threshold 1 -hls_flags delete_segments -f hls public/video.m3u8\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e인자에 대해 하나씩 알아보도록 하겠습니다. 소스코드에도 설명이 있습니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e-f v4l2\u003c/code\u003e: 입력 포맷을 v4l2(웹캠)으로 한다.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-i /dev/video0\u003c/code\u003e: \u003ccode\u003e/dev/video0\u003c/code\u003e(웹캠)에서 영상을 읽어온다.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-c:v libx264\u003c/code\u003e: 출력 비디오 포맷을 \u003ccode\u003elibx264\u003c/code\u003e로 한다.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-crf 23\u003c/code\u003e: Constant Rate Factor (CRF)를 23으로 한다. CRF는 영상의 품질과 관련된 인자로, 낮을수록 화질이 좋아지며, 23이 기본값이다.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-pix_fmt yuv420p\u003c/code\u003e: 픽셀 포맷을 Y:U:V = 4:2:0으로 설정한다. 이는 어떤 플레이어들이 이 포맷밖에 지원하지 않는 경우가 있기 때문으로, 실제로 이렇게 하지 않으면 크롬에서 비디오가 깨진다. \u003ca href=\"https://trac.ffmpeg.org/wiki/Encode/H.264\"\u003e설명 참조\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-hls_time 2\u003c/code\u003e: ts파일의 길이를 2초로 한다. (이유는 알 수 없지만 제 컴퓨터에서는 이 옵션이 제대로 작동하지 않고, 파일 길이가 약 8초였습니다.)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-hls_list_size 5\u003c/code\u003e: \u003ccode\u003e.m3u8\u003c/code\u003e파일에 포함할 \u003ccode\u003e.ts\u003c/code\u003e파일 리스트의 길이를 최대 5로 설정한다. 이는 실시간 스트리밍이기 때문에 필요한 옵션으로, 오래 전에 생성된 것들까지 포함해서 모든 segment들을 \u003ccode\u003e.m3u8\u003c/code\u003e파일에 포함하면 비효율적이므로 최근 5개의 segment만 포함하는 것이다.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-hls_delete_threshold 1\u003c/code\u003e: \u003ccode\u003e.m3u8\u003c/code\u003e에 포함되지 않은 segment를 1개까지 허용한다. 추후 설명.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-hls_flags delete_segments\u003c/code\u003e: 오래된 segment를 삭제한다.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-f hls public/video.m3u8\u003c/code\u003e: 출력 파일 이름. 이렇게 하면 \u003ccode\u003e.ts\u003c/code\u003e파일들은 \u003ccode\u003evideo.ts\u003c/code\u003e와 같이 생성된다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e위의 옵션 중 \u003ccode\u003e-hls_list_size\u003c/code\u003e, \u003ccode\u003e-hls_delete_threshold\u003c/code\u003e, \u003ccode\u003e-hls_flags delete_segments\u003c/code\u003e는 세 개가 한 세트인 옵션입니다. 약간 설명이 복잡한데, \u003ccode\u003e-hls_flags delete_segments\u003c/code\u003e옵션은 위에서 언급했듯 오래된 segment들을 지우는 옵션이고, \u003ccode\u003e-hls_delete_threshold\u003c/code\u003e옵션은 \u003ccode\u003e.m3u8\u003c/code\u003e에 포함되지 않은 segment들의 경우, 몇 개까지를 오래된 것으로 생각할 것인지를 지정하는 옵션입니다.\u003c/p\u003e\n\u003cp\u003e예를 들어, \u003ccode\u003ehls_list_size\u003c/code\u003e가 7이고, \u003ccode\u003ehls_delete_threshold\u003c/code\u003e가 3이라고 가정합니다. 그러면 \u003ccode\u003e.m3u8\u003c/code\u003e에는 n, n+1, n+2...n+6까지 총 7개의 segment에 대한 정보가 있을 것입니다. 이때 \u003ccode\u003ehls_delete_threshold\u003c/code\u003e가 3이기 때문에, n-1, n-2, n-3까지의 segment는 오래된 것으로 간주하지 않고, n-4부터를 오래된 것으로 간주합니다. 그러므로 \u003ccode\u003e.ts\u003c/code\u003e파일은 항상 10개가 존재하게 됩니다.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ehls_delete_threshold\u003c/code\u003e옵션이 필요한 이유는, 비록 \u003ccode\u003e.m3u8\u003c/code\u003e의 segment 리스트에서는 특정 segment가 지워졌지만, 인터넷 속도 등의 문제로 해당 segment를 계속 다운받고 있는 클라이언트가 있을 수도 있기 때문입니다.\u003c/p\u003e\n\u003cp\u003e이렇게 옵션을 주면 파일들이 실시간으로 업데이트되며, 클라이언트는 일반 HLS파일을 재생하듯 실시간 영상을 재생할 수 있습니다. 아래는 스크린샷입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/imgs/b1e1387535b43472a86e79e1db83a560.png\" alt=\"screenshot\"\u003e\u003c/p\u003e\n\u003cp\u003e웹캠에서 찍은 영상을 HTML로 스트리밍하여 볼 수 있습니다. (URL은 개인 서버를 사용 중이라 지웠습니다.)\u003c/p\u003e\n\u003ch1\u003e참고문헌\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://superuser.com/questions/714974/convert-rtmp-streaming-to-hls-streaming-using-ffmpeg\"\u003eFFmpeg RTMP HLS변환 방법\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e첫 번째 답변이 도움이 많이 되었습니다. 단, 아래 옵션 중 \u003ccode\u003e-hls_wrap\u003c/code\u003e은 deprecated되었습니다. 대신 제가 사용한 것처럼 \u003ccode\u003e-hls_delete_threshold 1 -hls_flags delete_segments\u003c/code\u003e를 사용하시면 됩니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003effmpeg -v verbose -i rtmp://host:port/stream -c:v libx264 -c:a aac -ac 1 -strict -2 -crf\n18 -profile:v baseline -maxrate 400k -bufsize 1835k -pix_fmt yuv420p -flags -global_header -hls_time\n10 -hls_list_size 6 -hls_wrap 10 -start_number 1 pathToFolderYouWantTo/streamName.m3u8\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://superuser.com/questions/677576/what-is-crf-used-for-in-ffmpeg\"\u003eCRF 설명\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://trac.ffmpeg.org/wiki/Capture/Webcam\"\u003eFFmpeg에서 웹캠 입력받는 방법\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://linuxize.com/post/how-to-install-ffmpeg-on-centos-8/\"\u003eCentOS에서 FFmpeg 설치 방법\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n","imageMapping":{"screenshot.png":"/imgs/b1e1387535b43472a86e79e1db83a560.png"},"name":"2020-08-11-ffmpeg"},"postsInSameCategory":[{"name":"2020-08-11-ffmpeg","title":"HTTP 웹캠 라이브 스트리밍 구현하기","date":"2020-08-11T04:03:18.931Z","category":"live streaming"}]},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"2020-08-11-ffmpeg"},"buildId":"5MaiEKf8969grUIlJXEoz","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>