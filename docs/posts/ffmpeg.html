<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/fcd26abcfb6841ad.css" as="style"/><link rel="stylesheet" href="/_next/static/css/fcd26abcfb6841ad.css" data-n-g=""/><link rel="preload" href="/_next/static/css/afa765f830d46ca8.css" as="style"/><link rel="stylesheet" href="/_next/static/css/afa765f830d46ca8.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-3b5a00d5d7e8d93b.js" defer=""></script><script src="/_next/static/chunks/main-a455256c0236c590.js" defer=""></script><script src="/_next/static/chunks/pages/_app-50a6bd5804e0fb30.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-5c46775951523a08.js" defer=""></script><script src="/_next/static/TEypFbFl9zt0OaBPC1cJx/_buildManifest.js" defer=""></script><script src="/_next/static/TEypFbFl9zt0OaBPC1cJx/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="header_header__n6agz"><h1>[UNKNOWN-PGR]</h1><div><a class="header_nav__JpY1D" href="/">MAIN</a><a class="header_nav__JpY1D" href="/about">ABOUT</a></div></div><main class="layout_main__j5lor"><div><h1 class="post_title__aQveY">HTTP 웹캠 라이브 스트리밍 구현하기</h1><main class="post_main__mmVmG"><p>이번에 어떤 프로젝트에 참여하면서 라이브 스트리밍을 구현해야 할 일이 생겼습니다. 어떤 카메라로부터 영상을 받아서 그것을 웹브라우저에서 확인할 수 있게 해야 하는 건데요. 구글링을 하면서 찾아보니, 이걸 깔끔하게 구현해주는 마땅한 솔루션이 없었습니다. 그래서 이걸 직접 구현해보기로 했습니다.</p>
<ul>
<li><a href="https://github.com/unknownpgr/node-webcam-streaming">깃허브 리포</a></li>
</ul>
<p>영상, 특히 라이브 스트리밍을 할 때 생기는 가장 큰 문제점은 도대체 어떻게 영상을 받을지, 그리고 어떻게 스트리밍할지입니다. 다행스럽게도 저는 이전에 비슷한 경험을 몇 번 해 보아서 두 가지 솔루션을 알고 있었습니다.</p>
<ol>
<li>OpenCV등을 이용, 카메라 <u>영상을 이미지로 변환 후 클라이언트에게 전송</u>. 클라이언트는 이미지를 계속 업데이트하여 보여줌. 그러므로 실은 영상이 아니라 빠르게 바뀌는 이미지에 불과함.</li>
<li><u><a href="https://ffmpeg.org/">FFmpeg</a></u>를 사용.</li>
</ol>
<p>언듯 생각하기에는 1번 솔루션은 상당히 단순무식해보입니다. 그러나 실제로 해 보면 꽤 영상 퀄리티와 FPS가 괜찮은데다, 구현하기도 편하고(인코딩 관련 고생할 필요가 없습니다), 특히 영상에 어떤 처리를 한 후 전송해야 할 때 유용합니다. 그러나 소리를 전송할 수 없는 단점이 있습니다. 그래서 이번에는 2번 솔루션을 구현해보기로 했습니다.</p>
<h1>이전에 했던 것</h1>
<p>예전에 FFmpeg를 사용할 때에는 실시간으로 영상 전송이 가능한 프로토콜을 잘 몰랐습니다. 그래서 상당히 로우레벨로 접근해야만 했었는데, 다음과 같은 방법을 사용했습니다.</p>
<ol>
<li>FFmpeg를 사용하여 웹캠 영상을 webm형식으로 변환, 출력 스트림을 <code>stdout</code>으로 pipe하면 Node.js 서버에서 그것을 받음.</li>
<li>Node.js서버에서는 받은 스트림을 버퍼에 담음.</li>
<li>버퍼가 다 차면 버퍼를 큐에 넣음.</li>
<li>만약 클라이언트가 영상을 요구할 경우, 서버는 큐에서 버퍼를 꺼내서 클라이언트에게 전송함.</li>
<li>클라이언트는 HTML5의 <a href="https://developer.mozilla.org/en-US/docs/Web/API/MediaSource">Media Source API</a>를 이용하여 버퍼를 video tag에 추가함.</li>
</ol>
<p>커밋 기록을 보니 이걸 구현한 게 19년 2월이었으니까, 대충 고등학교 졸업하기 직전이네요. 지금 와서 보니 상당히 비효율적인 구현입니다. 애초에 두 사람이 동시에 스트리밍을 받을 수 없는 구조네요.</p>
<h1>이번에 한 것</h1>
<p>이번에는 저렇게 하지 않고, FFmpeg의 기능을 충분히 이용하기로 했습니다. 코드가 정말 간단해서, 어쩌면 직접 <a href="https://github.com/unknownpgr/node-webcam-streaming/blob/master/index.js">소스코드</a>를 보시는 편이 더 이해가 잘 될 수도 있습니다.</p>
<ul>
<li>FFmpeg는 출력을 HTTP Live Streaming(HLS)로 할 수 있는데, 이렇게 하면 출력으로 <code>.m3u8</code>파일과 여러 <code>.ts</code>파일들이 생깁니다. <code>.ts</code>파일은 영상을 조각으로 나눠놓은 segment파일이며, <code>.m3u8</code>파일은 <code>.ts</code>파일들의 정보를 담고 있는 메타 파일입니다.</li>
<li>Node.js서버는 단지 저 파일들을 정적으로 서비스하기만 합니다.</li>
<li>클라이언트는 <a href="https://github.com/video-dev/hls.js#getting-started">hls.js</a> 라이브러리를 사용하여 영상을 <code>video</code>태그로 볼 수 있도록 합니다. 물론 일부 브라우저는 외부 라이브러리 없이 <code>video</code>태그의 <code>src</code>에 <code>.m3u8</code>파일을 넘겨줘도 잘 재생해줍니다. 그런데 가장 메이저한 브라우저인 크롬 데스크톱에서 이게 안 돼서 그냥 라이브러리를 쓰기로 했습니다.</li>
</ul>
<p>아래는 Node.js에서 실행하는 FFmpeg 커맨드를 그대로 옮겨 온 것입니다. FFmpeg가 설치되어있을 경우 터미널에 바로 입력하면 작동합니다.</p>
<pre><code class="language-bash">ffmpeg -f v4l2 -i /dev/video0 -c:v libx264 -crf 23 -pix_fmt yuv420p -hls_time 2 -hls_list_size 5 -hls_delete_threshold 1 -hls_flags delete_segments -f hls public/video.m3u8
</code></pre>
<p>인자에 대해 하나씩 알아보도록 하겠습니다. 소스코드에도 설명이 있습니다.</p>
<ul>
<li><code>-f v4l2</code>: 입력 포맷을 v4l2(웹캠)으로 한다.</li>
<li><code>-i /dev/video0</code>: <code>/dev/video0</code>(웹캠)에서 영상을 읽어온다.</li>
<li><code>-c:v libx264</code>: 출력 비디오 포맷을 <code>libx264</code>로 한다.</li>
<li><code>-crf 23</code>: Constant Rate Factor (CRF)를 23으로 한다. CRF는 영상의 품질과 관련된 인자로, 낮을수록 화질이 좋아지며, 23이 기본값이다.</li>
<li><code>-pix_fmt yuv420p</code>: 픽셀 포맷을 Y:U:V = 4:2:0으로 설정한다. 이는 어떤 플레이어들이 이 포맷밖에 지원하지 않는 경우가 있기 때문으로, 실제로 이렇게 하지 않으면 크롬에서 비디오가 깨진다. <a href="https://trac.ffmpeg.org/wiki/Encode/H.264">설명 참조</a>.</li>
<li><code>-hls_time 2</code>: ts파일의 길이를 2초로 한다. (이유는 알 수 없지만 제 컴퓨터에서는 이 옵션이 제대로 작동하지 않고, 파일 길이가 약 8초였습니다.)</li>
<li><code>-hls_list_size 5</code>: <code>.m3u8</code>파일에 포함할 <code>.ts</code>파일 리스트의 길이를 최대 5로 설정한다. 이는 실시간 스트리밍이기 때문에 필요한 옵션으로, 오래 전에 생성된 것들까지 포함해서 모든 segment들을 <code>.m3u8</code>파일에 포함하면 비효율적이므로 최근 5개의 segment만 포함하는 것이다.</li>
<li><code>-hls_delete_threshold 1</code>: <code>.m3u8</code>에 포함되지 않은 segment를 1개까지 허용한다. 추후 설명.</li>
<li><code>-hls_flags delete_segments</code>: 오래된 segment를 삭제한다.</li>
<li><code>-f hls public/video.m3u8</code>: 출력 파일 이름. 이렇게 하면 <code>.ts</code>파일들은 <code>video.ts</code>와 같이 생성된다.</li>
</ul>
<p>위의 옵션 중 <code>-hls_list_size</code>, <code>-hls_delete_threshold</code>, <code>-hls_flags delete_segments</code>는 세 개가 한 세트인 옵션입니다. 약간 설명이 복잡한데, <code>-hls_flags delete_segments</code>옵션은 위에서 언급했듯 오래된 segment들을 지우는 옵션이고, <code>-hls_delete_threshold</code>옵션은 <code>.m3u8</code>에 포함되지 않은 segment들의 경우, 몇 개까지를 오래된 것으로 생각할 것인지를 지정하는 옵션입니다.</p>
<p>예를 들어, <code>hls_list_size</code>가 7이고, <code>hls_delete_threshold</code>가 3이라고 가정합니다. 그러면 <code>.m3u8</code>에는 n, n+1, n+2...n+6까지 총 7개의 segment에 대한 정보가 있을 것입니다. 이때 <code>hls_delete_threshold</code>가 3이기 때문에, n-1, n-2, n-3까지의 segment는 오래된 것으로 간주하지 않고, n-4부터를 오래된 것으로 간주합니다. 그러므로 <code>.ts</code>파일은 항상 10개가 존재하게 됩니다.</p>
<p><code>hls_delete_threshold</code>옵션이 필요한 이유는, 비록 <code>.m3u8</code>의 segment 리스트에서는 특정 segment가 지워졌지만, 인터넷 속도 등의 문제로 해당 segment를 계속 다운받고 있는 클라이언트가 있을 수도 있기 때문입니다.</p>
<p>이렇게 옵션을 주면 파일들이 실시간으로 업데이트되며, 클라이언트는 일반 HLS파일을 재생하듯 실시간 영상을 재생할 수 있습니다. 아래는 스크린샷입니다.</p>
<p><img src="/imgs/b1e1387535b43472a86e79e1db83a560.png" alt="screenshot"></p>
<p>웹캠에서 찍은 영상을 HTML로 스트리밍하여 볼 수 있습니다. (URL은 개인 서버를 사용 중이라 지웠습니다.)</p>
<h1>참고문헌</h1>
<ul>
<li>
<p><a href="https://superuser.com/questions/714974/convert-rtmp-streaming-to-hls-streaming-using-ffmpeg">FFmpeg RTMP HLS변환 방법</a></p>
<ul>
<li>
<p>첫 번째 답변이 도움이 많이 되었습니다. 단, 아래 옵션 중 <code>-hls_wrap</code>은 deprecated되었습니다. 대신 제가 사용한 것처럼 <code>-hls_delete_threshold 1 -hls_flags delete_segments</code>를 사용하시면 됩니다.</p>
<pre><code class="language-bash">ffmpeg -v verbose -i rtmp://host:port/stream -c:v libx264 -c:a aac -ac 1 -strict -2 -crf
18 -profile:v baseline -maxrate 400k -bufsize 1835k -pix_fmt yuv420p -flags -global_header -hls_time
10 -hls_list_size 6 -hls_wrap 10 -start_number 1 pathToFolderYouWantTo/streamName.m3u8
</code></pre>
</li>
</ul>
</li>
<li>
<p><a href="https://superuser.com/questions/677576/what-is-crf-used-for-in-ffmpeg">CRF 설명</a></p>
</li>
<li>
<p><a href="https://trac.ffmpeg.org/wiki/Capture/Webcam">FFmpeg에서 웹캠 입력받는 방법</a></p>
</li>
<li>
<p><a href="https://linuxize.com/post/how-to-install-ffmpeg-on-centos-8/">CentOS에서 FFmpeg 설치 방법</a></p>
</li>
</ul>
</main><h1>Posts in <!-- -->live streaming<!-- --> cateogry</h1><ul><li class="post_listSelected__vtK9Y"><a href="/posts/ffmpeg">HTTP 웹캠 라이브 스트리밍 구현하기</a></li></ul></div></main><footer class="footer_footer__E_Uzl">© 2020 Copyright : Unknownpgr</footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"HTTP 웹캠 라이브 스트리밍 구현하기","category":"live streaming","date":"2020-08-11T04:03:18.931Z","html":"\u003cp\u003e이번에 어떤 프로젝트에 참여하면서 라이브 스트리밍을 구현해야 할 일이 생겼습니다. 어떤 카메라로부터 영상을 받아서 그것을 웹브라우저에서 확인할 수 있게 해야 하는 건데요. 구글링을 하면서 찾아보니, 이걸 깔끔하게 구현해주는 마땅한 솔루션이 없었습니다. 그래서 이걸 직접 구현해보기로 했습니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/unknownpgr/node-webcam-streaming\"\u003e깃허브 리포\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e영상, 특히 라이브 스트리밍을 할 때 생기는 가장 큰 문제점은 도대체 어떻게 영상을 받을지, 그리고 어떻게 스트리밍할지입니다. 다행스럽게도 저는 이전에 비슷한 경험을 몇 번 해 보아서 두 가지 솔루션을 알고 있었습니다.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eOpenCV등을 이용, 카메라 \u003cu\u003e영상을 이미지로 변환 후 클라이언트에게 전송\u003c/u\u003e. 클라이언트는 이미지를 계속 업데이트하여 보여줌. 그러므로 실은 영상이 아니라 빠르게 바뀌는 이미지에 불과함.\u003c/li\u003e\n\u003cli\u003e\u003cu\u003e\u003ca href=\"https://ffmpeg.org/\"\u003eFFmpeg\u003c/a\u003e\u003c/u\u003e를 사용.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e언듯 생각하기에는 1번 솔루션은 상당히 단순무식해보입니다. 그러나 실제로 해 보면 꽤 영상 퀄리티와 FPS가 괜찮은데다, 구현하기도 편하고(인코딩 관련 고생할 필요가 없습니다), 특히 영상에 어떤 처리를 한 후 전송해야 할 때 유용합니다. 그러나 소리를 전송할 수 없는 단점이 있습니다. 그래서 이번에는 2번 솔루션을 구현해보기로 했습니다.\u003c/p\u003e\n\u003ch1\u003e이전에 했던 것\u003c/h1\u003e\n\u003cp\u003e예전에 FFmpeg를 사용할 때에는 실시간으로 영상 전송이 가능한 프로토콜을 잘 몰랐습니다. 그래서 상당히 로우레벨로 접근해야만 했었는데, 다음과 같은 방법을 사용했습니다.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eFFmpeg를 사용하여 웹캠 영상을 webm형식으로 변환, 출력 스트림을 \u003ccode\u003estdout\u003c/code\u003e으로 pipe하면 Node.js 서버에서 그것을 받음.\u003c/li\u003e\n\u003cli\u003eNode.js서버에서는 받은 스트림을 버퍼에 담음.\u003c/li\u003e\n\u003cli\u003e버퍼가 다 차면 버퍼를 큐에 넣음.\u003c/li\u003e\n\u003cli\u003e만약 클라이언트가 영상을 요구할 경우, 서버는 큐에서 버퍼를 꺼내서 클라이언트에게 전송함.\u003c/li\u003e\n\u003cli\u003e클라이언트는 HTML5의 \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/API/MediaSource\"\u003eMedia Source API\u003c/a\u003e를 이용하여 버퍼를 video tag에 추가함.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e커밋 기록을 보니 이걸 구현한 게 19년 2월이었으니까, 대충 고등학교 졸업하기 직전이네요. 지금 와서 보니 상당히 비효율적인 구현입니다. 애초에 두 사람이 동시에 스트리밍을 받을 수 없는 구조네요.\u003c/p\u003e\n\u003ch1\u003e이번에 한 것\u003c/h1\u003e\n\u003cp\u003e이번에는 저렇게 하지 않고, FFmpeg의 기능을 충분히 이용하기로 했습니다. 코드가 정말 간단해서, 어쩌면 직접 \u003ca href=\"https://github.com/unknownpgr/node-webcam-streaming/blob/master/index.js\"\u003e소스코드\u003c/a\u003e를 보시는 편이 더 이해가 잘 될 수도 있습니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFFmpeg는 출력을 HTTP Live Streaming(HLS)로 할 수 있는데, 이렇게 하면 출력으로 \u003ccode\u003e.m3u8\u003c/code\u003e파일과 여러 \u003ccode\u003e.ts\u003c/code\u003e파일들이 생깁니다. \u003ccode\u003e.ts\u003c/code\u003e파일은 영상을 조각으로 나눠놓은 segment파일이며, \u003ccode\u003e.m3u8\u003c/code\u003e파일은 \u003ccode\u003e.ts\u003c/code\u003e파일들의 정보를 담고 있는 메타 파일입니다.\u003c/li\u003e\n\u003cli\u003eNode.js서버는 단지 저 파일들을 정적으로 서비스하기만 합니다.\u003c/li\u003e\n\u003cli\u003e클라이언트는 \u003ca href=\"https://github.com/video-dev/hls.js#getting-started\"\u003ehls.js\u003c/a\u003e 라이브러리를 사용하여 영상을 \u003ccode\u003evideo\u003c/code\u003e태그로 볼 수 있도록 합니다. 물론 일부 브라우저는 외부 라이브러리 없이 \u003ccode\u003evideo\u003c/code\u003e태그의 \u003ccode\u003esrc\u003c/code\u003e에 \u003ccode\u003e.m3u8\u003c/code\u003e파일을 넘겨줘도 잘 재생해줍니다. 그런데 가장 메이저한 브라우저인 크롬 데스크톱에서 이게 안 돼서 그냥 라이브러리를 쓰기로 했습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e아래는 Node.js에서 실행하는 FFmpeg 커맨드를 그대로 옮겨 온 것입니다. FFmpeg가 설치되어있을 경우 터미널에 바로 입력하면 작동합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003effmpeg -f v4l2 -i /dev/video0 -c:v libx264 -crf 23 -pix_fmt yuv420p -hls_time 2 -hls_list_size 5 -hls_delete_threshold 1 -hls_flags delete_segments -f hls public/video.m3u8\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e인자에 대해 하나씩 알아보도록 하겠습니다. 소스코드에도 설명이 있습니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e-f v4l2\u003c/code\u003e: 입력 포맷을 v4l2(웹캠)으로 한다.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-i /dev/video0\u003c/code\u003e: \u003ccode\u003e/dev/video0\u003c/code\u003e(웹캠)에서 영상을 읽어온다.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-c:v libx264\u003c/code\u003e: 출력 비디오 포맷을 \u003ccode\u003elibx264\u003c/code\u003e로 한다.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-crf 23\u003c/code\u003e: Constant Rate Factor (CRF)를 23으로 한다. CRF는 영상의 품질과 관련된 인자로, 낮을수록 화질이 좋아지며, 23이 기본값이다.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-pix_fmt yuv420p\u003c/code\u003e: 픽셀 포맷을 Y:U:V = 4:2:0으로 설정한다. 이는 어떤 플레이어들이 이 포맷밖에 지원하지 않는 경우가 있기 때문으로, 실제로 이렇게 하지 않으면 크롬에서 비디오가 깨진다. \u003ca href=\"https://trac.ffmpeg.org/wiki/Encode/H.264\"\u003e설명 참조\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-hls_time 2\u003c/code\u003e: ts파일의 길이를 2초로 한다. (이유는 알 수 없지만 제 컴퓨터에서는 이 옵션이 제대로 작동하지 않고, 파일 길이가 약 8초였습니다.)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-hls_list_size 5\u003c/code\u003e: \u003ccode\u003e.m3u8\u003c/code\u003e파일에 포함할 \u003ccode\u003e.ts\u003c/code\u003e파일 리스트의 길이를 최대 5로 설정한다. 이는 실시간 스트리밍이기 때문에 필요한 옵션으로, 오래 전에 생성된 것들까지 포함해서 모든 segment들을 \u003ccode\u003e.m3u8\u003c/code\u003e파일에 포함하면 비효율적이므로 최근 5개의 segment만 포함하는 것이다.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-hls_delete_threshold 1\u003c/code\u003e: \u003ccode\u003e.m3u8\u003c/code\u003e에 포함되지 않은 segment를 1개까지 허용한다. 추후 설명.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-hls_flags delete_segments\u003c/code\u003e: 오래된 segment를 삭제한다.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-f hls public/video.m3u8\u003c/code\u003e: 출력 파일 이름. 이렇게 하면 \u003ccode\u003e.ts\u003c/code\u003e파일들은 \u003ccode\u003evideo.ts\u003c/code\u003e와 같이 생성된다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e위의 옵션 중 \u003ccode\u003e-hls_list_size\u003c/code\u003e, \u003ccode\u003e-hls_delete_threshold\u003c/code\u003e, \u003ccode\u003e-hls_flags delete_segments\u003c/code\u003e는 세 개가 한 세트인 옵션입니다. 약간 설명이 복잡한데, \u003ccode\u003e-hls_flags delete_segments\u003c/code\u003e옵션은 위에서 언급했듯 오래된 segment들을 지우는 옵션이고, \u003ccode\u003e-hls_delete_threshold\u003c/code\u003e옵션은 \u003ccode\u003e.m3u8\u003c/code\u003e에 포함되지 않은 segment들의 경우, 몇 개까지를 오래된 것으로 생각할 것인지를 지정하는 옵션입니다.\u003c/p\u003e\n\u003cp\u003e예를 들어, \u003ccode\u003ehls_list_size\u003c/code\u003e가 7이고, \u003ccode\u003ehls_delete_threshold\u003c/code\u003e가 3이라고 가정합니다. 그러면 \u003ccode\u003e.m3u8\u003c/code\u003e에는 n, n+1, n+2...n+6까지 총 7개의 segment에 대한 정보가 있을 것입니다. 이때 \u003ccode\u003ehls_delete_threshold\u003c/code\u003e가 3이기 때문에, n-1, n-2, n-3까지의 segment는 오래된 것으로 간주하지 않고, n-4부터를 오래된 것으로 간주합니다. 그러므로 \u003ccode\u003e.ts\u003c/code\u003e파일은 항상 10개가 존재하게 됩니다.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ehls_delete_threshold\u003c/code\u003e옵션이 필요한 이유는, 비록 \u003ccode\u003e.m3u8\u003c/code\u003e의 segment 리스트에서는 특정 segment가 지워졌지만, 인터넷 속도 등의 문제로 해당 segment를 계속 다운받고 있는 클라이언트가 있을 수도 있기 때문입니다.\u003c/p\u003e\n\u003cp\u003e이렇게 옵션을 주면 파일들이 실시간으로 업데이트되며, 클라이언트는 일반 HLS파일을 재생하듯 실시간 영상을 재생할 수 있습니다. 아래는 스크린샷입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/imgs/b1e1387535b43472a86e79e1db83a560.png\" alt=\"screenshot\"\u003e\u003c/p\u003e\n\u003cp\u003e웹캠에서 찍은 영상을 HTML로 스트리밍하여 볼 수 있습니다. (URL은 개인 서버를 사용 중이라 지웠습니다.)\u003c/p\u003e\n\u003ch1\u003e참고문헌\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://superuser.com/questions/714974/convert-rtmp-streaming-to-hls-streaming-using-ffmpeg\"\u003eFFmpeg RTMP HLS변환 방법\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e첫 번째 답변이 도움이 많이 되었습니다. 단, 아래 옵션 중 \u003ccode\u003e-hls_wrap\u003c/code\u003e은 deprecated되었습니다. 대신 제가 사용한 것처럼 \u003ccode\u003e-hls_delete_threshold 1 -hls_flags delete_segments\u003c/code\u003e를 사용하시면 됩니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003effmpeg -v verbose -i rtmp://host:port/stream -c:v libx264 -c:a aac -ac 1 -strict -2 -crf\n18 -profile:v baseline -maxrate 400k -bufsize 1835k -pix_fmt yuv420p -flags -global_header -hls_time\n10 -hls_list_size 6 -hls_wrap 10 -start_number 1 pathToFolderYouWantTo/streamName.m3u8\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://superuser.com/questions/677576/what-is-crf-used-for-in-ffmpeg\"\u003eCRF 설명\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://trac.ffmpeg.org/wiki/Capture/Webcam\"\u003eFFmpeg에서 웹캠 입력받는 방법\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://linuxize.com/post/how-to-install-ffmpeg-on-centos-8/\"\u003eCentOS에서 FFmpeg 설치 방법\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n","name":"ffmpeg"},"postsInSameCategory":[{"title":"HTTP 웹캠 라이브 스트리밍 구현하기","category":"live streaming","date":"2020-08-11T04:03:18.931Z","name":"ffmpeg"}]},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"ffmpeg"},"buildId":"TEypFbFl9zt0OaBPC1cJx","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>